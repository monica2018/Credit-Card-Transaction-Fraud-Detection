{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Transaction Fraud Detection Project\n",
    "The purpose of this project is to build a fraud prediction model to identify credit card transaction frauds in the type “P” transact records in a government card transaction data set. Data contains around 100,000 records of card information, date, merchant information, location information of each transaction.\n",
    "\n",
    "We filled in the missing values in the dataset using median values of specific groups, then we created more than 300 variables (amount variables, frequency variables, day since variables) based on the original dataset.\n",
    "\n",
    "To reduce the dimensionality of the dataset, we first computed the KS (Kolmogorov-Smirnov\n",
    ") score and FDR (False Detection Rate) and dropped half of the variables based on the combination ranking of these two scores. Further, we used backward stepwise logistic regression algorithm to select 20 variables for our model fitting process.\n",
    "\n",
    "For the training and test set, after we dropped the out-of-date records, we random sampled 10 times of training and test set in order to better test the performance of each model. Then we used logistic regression model, boosted tree model, random forest model and support vector machine model to fit the training sets and compared the prediction outcomes on test sets of these models. We found that random forest model yielded the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Cardnum</th>\n",
       "      <th>Date</th>\n",
       "      <th>Merchnum</th>\n",
       "      <th>Merch description</th>\n",
       "      <th>Merch state</th>\n",
       "      <th>Merch zip</th>\n",
       "      <th>Transtype</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5142190439</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>5509006296254</td>\n",
       "      <td>FEDEX SHP 12/23/09 AB#</td>\n",
       "      <td>TN</td>\n",
       "      <td>38118.0</td>\n",
       "      <td>P</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5142183973</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>61003026333</td>\n",
       "      <td>SERVICE MERCHANDISE #81</td>\n",
       "      <td>MA</td>\n",
       "      <td>1803.0</td>\n",
       "      <td>P</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5142131721</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>4503082993600</td>\n",
       "      <td>OFFICE DEPOT #191</td>\n",
       "      <td>MD</td>\n",
       "      <td>20706.0</td>\n",
       "      <td>P</td>\n",
       "      <td>178.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5142148452</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>5509006296254</td>\n",
       "      <td>FEDEX SHP 12/28/09 AB#</td>\n",
       "      <td>TN</td>\n",
       "      <td>38118.0</td>\n",
       "      <td>P</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5142190439</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>5509006296254</td>\n",
       "      <td>FEDEX SHP 12/23/09 AB#</td>\n",
       "      <td>TN</td>\n",
       "      <td>38118.0</td>\n",
       "      <td>P</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recnum     Cardnum       Date       Merchnum        Merch description  \\\n",
       "0       1  5142190439 2010-01-01  5509006296254   FEDEX SHP 12/23/09 AB#   \n",
       "1       2  5142183973 2010-01-01    61003026333  SERVICE MERCHANDISE #81   \n",
       "2       3  5142131721 2010-01-01  4503082993600        OFFICE DEPOT #191   \n",
       "3       4  5142148452 2010-01-01  5509006296254   FEDEX SHP 12/28/09 AB#   \n",
       "4       5  5142190439 2010-01-01  5509006296254   FEDEX SHP 12/23/09 AB#   \n",
       "\n",
       "  Merch state  Merch zip Transtype  Amount  Fraud  \n",
       "0          TN    38118.0         P    3.62      0  \n",
       "1          MA     1803.0         P   31.42      0  \n",
       "2          MD    20706.0         P  178.49      0  \n",
       "3          TN    38118.0         P    3.62      0  \n",
       "4          TN    38118.0         P    3.62      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "card= pd.read_excel('Raw data_card transactions.xlsx')\n",
    "card = card[card['Transtype']=='P'] #We only study the Transtype P here\n",
    "card.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96398 entries, 0 to 96752\n",
      "Data columns (total 10 columns):\n",
      "Recnum               96398 non-null int64\n",
      "Cardnum              96398 non-null int64\n",
      "Date                 96398 non-null datetime64[ns]\n",
      "Merchnum             93199 non-null object\n",
      "Merch description    96398 non-null object\n",
      "Merch state          95377 non-null object\n",
      "Merch zip            92097 non-null float64\n",
      "Transtype            96398 non-null object\n",
      "Amount               96398 non-null float64\n",
      "Fraud                96398 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(2), int64(3), object(4)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "card.info() #There are NAs in Merch state, Merch zip\n",
    "## We don't want to study Merchnum, since this field and Merch description are both unique identifier for merchants\n",
    "## while Merch description doesn't have NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add values into zip by major corresponding values group by merch description, new field is zip_x\n",
    "merNum_Des = card.dropna(axis=0).groupby('Merch description').agg({'Merch zip':lambda x:x.value_counts().index[0]})\n",
    "newcard = card.merge(merNum_Des,right_index=True,left_on='Merch description',how='left')\n",
    "newcard['Merch zip_x'].fillna(newcard['Merch zip_y'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add values into zip by major corresponding values group by cardnum, new field is zip_x_x\n",
    "merNum_Des = newcard.dropna(axis=0).groupby('Cardnum').agg({'Merch zip_x':lambda x:x.value_counts().index[0]})\n",
    "newcard = newcard.merge(merNum_Des,right_index=True,left_on='Cardnum',how='left')\n",
    "newcard['Merch zip_x_x'].fillna(newcard['Merch zip_x_y'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96398 entries, 0 to 96752\n",
      "Data columns (total 12 columns):\n",
      "Recnum               96398 non-null int64\n",
      "Cardnum              96398 non-null int64\n",
      "Date                 96398 non-null datetime64[ns]\n",
      "Merchnum             93199 non-null object\n",
      "Merch description    96398 non-null object\n",
      "Merch state          95377 non-null object\n",
      "Merch zip_x_x        96344 non-null float64\n",
      "Transtype            96398 non-null object\n",
      "Amount               96398 non-null float64\n",
      "Fraud                96398 non-null int64\n",
      "Merch zip_y          93185 non-null float64\n",
      "Merch zip_x_y        96334 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(4), int64(3), object(4)\n",
      "memory usage: 9.6+ MB\n"
     ]
    }
   ],
   "source": [
    "newcard.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96398 entries, 0 to 96752\n",
      "Data columns (total 13 columns):\n",
      "Recnum               96398 non-null int64\n",
      "Cardnum              96398 non-null int64\n",
      "Date                 96398 non-null datetime64[ns]\n",
      "Merchnum             93199 non-null object\n",
      "Merch description    96398 non-null object\n",
      "Merch state_x        96290 non-null object\n",
      "Merch zip_x_x        96344 non-null float64\n",
      "Transtype            96398 non-null object\n",
      "Amount               96398 non-null float64\n",
      "Fraud                96398 non-null int64\n",
      "Merch zip_y          93185 non-null float64\n",
      "Merch zip_x_y        96334 non-null float64\n",
      "Merch state_y        96277 non-null object\n",
      "dtypes: datetime64[ns](1), float64(4), int64(3), object(5)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "## Add values into state by major corresponding values group by zip, new field is state_x\n",
    "a = newcard[newcard['Merch zip_x_x'].notna()] \n",
    "b = a[a['Merch state'].notna()] ## b is data without NAs in zip_x_x and state\n",
    "merNum_Des = b.groupby('Merch zip_x_x').agg({'Merch state':lambda x:x.value_counts().index[0]})\n",
    "newcard = newcard.merge(merNum_Des,right_index=True,left_on='Merch zip_x_x',how='left')\n",
    "newcard['Merch state_x'].fillna(newcard['Merch state_y'],inplace=True)\n",
    "newcard.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Cardnum</th>\n",
       "      <th>Date</th>\n",
       "      <th>Merchnum</th>\n",
       "      <th>Merch description</th>\n",
       "      <th>Merch state_x</th>\n",
       "      <th>Merch zip_x_x</th>\n",
       "      <th>Transtype</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Merch zip_y</th>\n",
       "      <th>Merch zip_x_y</th>\n",
       "      <th>Merch state_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>3259</td>\n",
       "      <td>5142153880</td>\n",
       "      <td>2010-01-14</td>\n",
       "      <td>582582822587</td>\n",
       "      <td>DIGITAL TECHNOLOGY CONTRA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>926.0</td>\n",
       "      <td>P</td>\n",
       "      <td>2340.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20746.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>3263</td>\n",
       "      <td>5142154098</td>\n",
       "      <td>2010-01-14</td>\n",
       "      <td>582582822587</td>\n",
       "      <td>DIGITAL TECHNOLOGY CONTRA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>926.0</td>\n",
       "      <td>P</td>\n",
       "      <td>2387.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20639.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3540</th>\n",
       "      <td>3541</td>\n",
       "      <td>5142154098</td>\n",
       "      <td>2010-01-17</td>\n",
       "      <td>582582822587</td>\n",
       "      <td>DIGITAL TECHNOLOGY CONTRA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>926.0</td>\n",
       "      <td>P</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20639.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3642</th>\n",
       "      <td>3643</td>\n",
       "      <td>5142153880</td>\n",
       "      <td>2010-01-17</td>\n",
       "      <td>582582822587</td>\n",
       "      <td>DIGITAL TECHNOLOGY CONTRA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>926.0</td>\n",
       "      <td>P</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20746.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>4970</td>\n",
       "      <td>5142194136</td>\n",
       "      <td>2010-01-24</td>\n",
       "      <td>597597721468</td>\n",
       "      <td>CRISTALIA ACQUISITION COR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>929.0</td>\n",
       "      <td>P</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90640.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Recnum     Cardnum       Date      Merchnum          Merch description  \\\n",
       "3258    3259  5142153880 2010-01-14  582582822587  DIGITAL TECHNOLOGY CONTRA   \n",
       "3262    3263  5142154098 2010-01-14  582582822587  DIGITAL TECHNOLOGY CONTRA   \n",
       "3540    3541  5142154098 2010-01-17  582582822587  DIGITAL TECHNOLOGY CONTRA   \n",
       "3642    3643  5142153880 2010-01-17  582582822587  DIGITAL TECHNOLOGY CONTRA   \n",
       "4969    4970  5142194136 2010-01-24  597597721468  CRISTALIA ACQUISITION COR   \n",
       "\n",
       "     Merch state_x  Merch zip_x_x Transtype  Amount  Fraud  Merch zip_y  \\\n",
       "3258           NaN          926.0         P  2340.0      0          NaN   \n",
       "3262           NaN          926.0         P  2387.0      0          NaN   \n",
       "3540           NaN          926.0         P  2300.0      0          NaN   \n",
       "3642           NaN          926.0         P  2500.0      0          NaN   \n",
       "4969           NaN          929.0         P    83.0      0          NaN   \n",
       "\n",
       "      Merch zip_x_y Merch state_y  \n",
       "3258        20746.0           NaN  \n",
       "3262        20639.0           NaN  \n",
       "3540        20639.0           NaN  \n",
       "3642        20746.0           NaN  \n",
       "4969        90640.0           NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcard[newcard['Merch state_x'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We searched the missing value in Zip on Google and find their corresponding State.\n",
    "dict = {\"907.0\":\"PR\", \"922.0\":\"PR\", \"920.0\":\"PR\", \"801.0\":\"USVI\",\"31040.0\":\"GA\", \"41160.0\":\"KY\", \"934.0\": \"PR\",\n",
    "\"902.0\": \"PR\", \"738.0\": \"PR\", \"90805.0\": \"CA\", \"76302.0\": \"TX\", \"914.0\": \"PR\", \"95461.0\": \"CA\", \"50823.0\": \"Other\", \n",
    "'926.0': \"PR\", '929.0':\"PR\", '1400.0':\"Other\", '65132.0':\"Other\", '86899.0':\"Other\", '23080.0':\"Other\",\n",
    "'60528.0':\"Other\", \"48700.0\": \"CA\", \"680.0\": \"PR\", \"681.0\": \"PR\", \"623.0\": \"PR\", \"726.0\": \"PR\", \"936.0\": \"PR\",\n",
    "\"791.0\": \"PR\", \"12108.0\": \"Other\", \"nan\":'Other'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## We used the previous dictionary to fill in missing fields in state.\n",
    "ab = newcard[newcard['Merch state_x'].isna()]\n",
    "ab['Merch zip_x_x']=ab['Merch zip_x_x'].astype('str')\n",
    "for i in range(len(ab['Merch zip_x_x'])):\n",
    "    ab['Merch state_x'].iloc[i]=dict[ab['Merch zip_x_x'].iloc[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We replace the values in newcard dataset with filled data\n",
    "ac = newcard[newcard['Merch state_x'].notna()][['Merch state_x']]\n",
    "ad = pd.concat([ab[['Merch state_x']],ac])\n",
    "newcard['Merch state_x'] = ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Cardnum</th>\n",
       "      <th>Date</th>\n",
       "      <th>Merchnum</th>\n",
       "      <th>Merch description</th>\n",
       "      <th>Merch state_x</th>\n",
       "      <th>Merch zip_x_x</th>\n",
       "      <th>Transtype</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Merch zip_y</th>\n",
       "      <th>Merch zip_x_y</th>\n",
       "      <th>Merch state_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Recnum, Cardnum, Date, Merchnum, Merch description, Merch state_x, Merch zip_x_x, Transtype, Amount, Fraud, Merch zip_y, Merch zip_x_y, Merch state_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## No NAs in field state now\n",
    "newcard[newcard['Merch state_x'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcard['Merch zip_x_x'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful columns and change the column name\n",
    "fdata = newcard[[\"Recnum\",\"Cardnum\",\"Date\",\"Merch description\",\"Merch state_x\",\"Merch zip_x_x\",\"Transtype\",\"Amount\",\"Fraud\"]]\n",
    "fdata.columns = [\"Recnum\",\"Cardnum\",\"Date\",\"Merch description\",\"Merch state\",\"Merch zip\",\"Transtype\",\"Amount\",\"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96398 entries, 0 to 96752\n",
      "Data columns (total 9 columns):\n",
      "Recnum               96398 non-null int64\n",
      "Cardnum              96398 non-null int64\n",
      "Date                 96398 non-null datetime64[ns]\n",
      "Merch description    96398 non-null object\n",
      "Merch state          96398 non-null object\n",
      "Merch zip            96398 non-null float64\n",
      "Transtype            96398 non-null object\n",
      "Amount               96398 non-null float64\n",
      "Fraud                96398 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(2), int64(3), object(3)\n",
      "memory usage: 7.4+ MB\n"
     ]
    }
   ],
   "source": [
    "fdata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Creating variables is accomplished by using the \"data table\" package in R. The R codes below can **generate 300 rolling window variables within 30 seconds**. Usually, it took at least 15 minutes for an efficient algorithm (as showed below) written in Python to do the same thing (without any package). The graphs shows what the 300 variables mean.\n",
    "<img src=\"png1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##  Cardnum, Merchnum\n",
    "for groupbyvar in ['Cardnum', 'Merchnum']:\n",
    "    data_sorted = data_sorted.sort_values(by = [groupbyvar, 'Date'])\n",
    "    data_sorted_index = data_sorted.set_index('Date’)\n",
    "    for agg in ['mean', 'max', 'median', 'sum', 'count']: \n",
    "        for days in ['1d', '3d', '7d', '14d', '30d']:\n",
    "            data_sorted[agg + '_' + groupbyvar + \"_\" + days]=getattr(data_sorted_index.groupby(groupbyvar)['Amount'].rolling(days),agg)().values\n",
    "            data_sorted['Actual/' + agg + \"_\" + groupbyvar + \"_\" + days] = data_sorted['Amount']/data_sorted[agg + '_' + groupbyvar + \"_\" + days]\n",
    "\n",
    "##  Cardnum and Merchnum, Cardnum and Zip, Cardnum and state\n",
    "for groupbyvar in ['Merchnum', 'Merch zip', 'Merch state']:\n",
    "    data_sorted = data_sorted.sort_values(by = ['Cardnum',groupbyvar, 'Date'])\n",
    "    data_sorted_index = data_sorted.set_index('Date')\n",
    "    for agg in ['mean', 'max', 'median', 'sum', 'count']: \n",
    "        for days in ['1d', '3d', '7d', '14d', '30d']:\n",
    "            data_sorted[agg + '_' + \"Cardnum_\" + groupbyvar + \"_\" + days] = getattr(data_sorted_index.groupby(['Cardnum',groupbyvar])['Amount'].rolling(days),agg)().values\n",
    "            data_sorted['Actual/' + agg + \"_\" + \"Cardnum_\" + groupbyvar + \"_\" + days] = data_sorted['Amount']/data_sorted[agg + '_' + \"Cardnum_\" + groupbyvar + \"_\" + days]            \n",
    "\n",
    "##  Create days since variables\n",
    "for groupbyvar in [['Cardnum'], ['Merchnum'], ['Cardnum', 'Merchnum'], ['Cardnum', 'Merch zip'], ['Cardnum', 'Merch state']]:\n",
    "    sortCols = groupbyvar[:]\n",
    "    sortCols.append('Date')\n",
    "    data_sorted1 = data_sorted1.sort_values(by = sortCols)\n",
    "    if len(groupbyvar) == 1:\n",
    "        data_sorted1['Days_since_per_' + groupbyvar[0]] = data_sorted1.groupby(groupbyvar)['Date'].apply(lambda x: (x - x.shift(1)).astype('timedelta64[D]')).fillna(365).values \n",
    "    else:\n",
    "        data_sorted1['Days_since_per_Cardnum_' + groupbyvar[1]] = data_sorted1.groupby(groupbyvar)['Date'].apply(lambda x: (x -x.shift(1)).astype('timedelta64[D]')).fillna(365).values \n",
    "\n",
    "                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions Reduction\n",
    "Since this is a fraud detection project and we have 300 variables, we decided to utilize **univariate Kolmogorov-Smirnov (KS)** and **Fraud Detection Rate (FDR) at top 3%** to filter out un-important variables. Then a stepwise regression model was implemented to do feature selection.\n",
    "### 1. Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "##import the 300 variables created by R codes along with the original 10 variables contained in the dataset\n",
    "mydata = pd.read_csv('var310.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "goods=mydata[mydata['Fraud']==0]\n",
    "bads=mydata[mydata['Fraud']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KS score for each variable\n",
    "from scipy.stats import ks_2samp\n",
    "KS=[]\n",
    "i=0\n",
    "for column in mydata.columns[11:]:\n",
    "    KS.append([ks_2samp(goods[column],bads[column])[0],column])\n",
    "    i = i+1\n",
    "KS_df = pd.DataFrame(KS, columns=['KS_score','variables']).sort_values(by='KS_score',ascending=False).set_index('variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FDR score for each variable\n",
    "fdrdic = {}\n",
    "for column in mydata.columns[11:]:\n",
    "    a = mydata[['Fraud',column]]\n",
    "    fdr=a.sort_values(column,ascending = False)['Fraud'].iloc[:round(0.03*len(mydata)),].sum()/len(bads)\n",
    "    fdrdic[column] = fdr\n",
    "fdr_df = pd.DataFrame.from_dict(fdrdic,orient = 'index')\n",
    "fdr_df.columns=['FDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDR_KS_df = fdr_df.merge(KS_df,left_index=True,right_index=True,how='left')\n",
    "FDR_KS_df['sum'] = FDR_KS_df['FDR']+FDR_KS_df['KS_score']\n",
    "top150 = FDR_KS_df.sort_values(by='sum',ascending=False)[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = list(top150.index)\n",
    "var.append('Fraud')\n",
    "top150_df = mydata[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stepwise - backward lgistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = top150_df.loc[:, top150_df.columns != 'Fraud']\n",
    "y=top150_df['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "model = LogisticRegression()\n",
    "rfecv = RFECV(estimator = model, step=1, cv = 3, verbose=3,n_jobs =-1, scoring='roc_auc')\n",
    "rfecv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(range(1,len(rfecv.grid_scores_) +1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_selected2 = pd.DataFrame(sorted(zip(map(lambda x: round(x), rfecv.ranking_),X.columns)), columns =['ranking','variables'])\n",
    "pd.options.display.max_rows = 60\n",
    "print(var_selected2)\n",
    "cols_keep=list(var_selected2['variables'][0:60])\n",
    "cols_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep=list(var_selected2['variables'][0:60])\n",
    "cols_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with linear and non-linear machine learning models\n",
    "We tried four kinds of machine learning models to do model selection, namely random forest, neural networks, gradient boosting tree and logistic regression. Each model was runned 10 times to mitigate the small sample size problem. Some visulization are made to show different model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[cols_keep]\n",
    "#Set up a out-of-time validation set\n",
    "tt_n = 84280\n",
    "x_ood = X.iloc[tt_n:,:]\n",
    "y_ood = y[tt_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "RF_train_total=[]\n",
    "RF_train_FDT=[]\n",
    "RF_train_FPT=[]\n",
    "RF_test_total=[]\n",
    "RF_test_FDT=[]\n",
    "RF_test_FPT=[]\n",
    "RF_OOD_total=[]\n",
    "RF_OOD_FDT=[]\n",
    "RF_OOD_FPT=[]\n",
    "for i in range(10):\n",
    "    RF_train=[]\n",
    "    RF_train_FP=[]\n",
    "    RF_train_FD=[]\n",
    "    RF_test=[]\n",
    "    RF_test_FP=[]\n",
    "    RF_test_FD=[]\n",
    "    RF_OOD=[]\n",
    "    RF_OOD_FP=[]\n",
    "    RF_OOD_FD=[]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25)\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train, y_train)  \n",
    "    \n",
    "    y_pred1 = clf.predict(X_train)\n",
    "    RF_FDR1 = pd.DataFrame(y_train)\n",
    "    RF_FDR1['score'] = clf.predict_proba(X_train)[:,1]\n",
    "    RF_FDR1['pred_y'] = y_pred1\n",
    "    RF_FDR1 = RF_FDR1.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    \n",
    "    y_pred2 = clf.predict(X_test)\n",
    "    RF_FDR2 = pd.DataFrame(y_test)\n",
    "    RF_FDR2['score'] = clf.predict_proba(X_test)[:,1]\n",
    "    RF_FDR2['pred_y'] = y_pred2\n",
    "    RF_FDR2 = RF_FDR2.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    y_pred3 = clf.predict(x_ood)\n",
    "    RF_FDR3 = pd.DataFrame(y_ood)\n",
    "    RF_FDR3['score'] = clf.predict_proba(x_ood)[:,1]\n",
    "    RF_FDR3['pred_y'] = y_pred3\n",
    "    RF_FDR3 = RF_FDR3.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    for j in range(1,101):\n",
    "        RF_OOD.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,0])/np.sum(RF_FDR3.iloc[:,0]))\n",
    "        RF_OOD_FD.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,0]))\n",
    "        RF_OOD_FP.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,:]['pred_y']-RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,:]['Fraud']==1))\n",
    "        RF_train.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,0])/np.sum(RF_FDR1.iloc[:,0]))\n",
    "        RF_train_FD.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,0]))\n",
    "        RF_train_FP.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,:]['pred_y']-RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,:]['Fraud']==1))\n",
    "        RF_test.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,0])/np.sum(RF_FDR2.iloc[:,0]))\n",
    "        RF_test_FD.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,0]))\n",
    "        RF_test_FP.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,:]['pred_y']-RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,:]['Fraud']==1))\n",
    "    RF_train_total.append(RF_train)\n",
    "    RF_train_FPT.append(RF_train_FP)\n",
    "    RF_train_FDT.append(RF_train_FD)                      \n",
    "    RF_test_total.append(RF_test)\n",
    "    RF_test_FPT.append(RF_test_FP)\n",
    "    RF_test_FDT.append(RF_test_FD)\n",
    "    RF_OOD_total.append(RF_OOD)\n",
    "    RF_OOD_FPT.append(RF_OOD_FP)\n",
    "    RF_OOD_FDT.append(RF_OOD_FD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "RF_train_total=[]\n",
    "RF_train_FDT=[]\n",
    "RF_train_FPT=[]\n",
    "RF_test_total=[]\n",
    "RF_test_FDT=[]\n",
    "RF_test_FPT=[]\n",
    "RF_OOD_total=[]\n",
    "RF_OOD_FDT=[]\n",
    "RF_OOD_FPT=[]\n",
    "for i in range(10):\n",
    "    RF_train=[]\n",
    "    RF_train_FP=[]\n",
    "    RF_train_FD=[]\n",
    "    RF_test=[]\n",
    "    RF_test_FP=[]\n",
    "    RF_test_FD=[]\n",
    "    RF_OOD=[]\n",
    "    RF_OOD_FP=[]\n",
    "    RF_OOD_FD=[]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25)\n",
    "    \n",
    "    clf = MLPClassifier(hidden_layer_sizes=(10,5))\n",
    "    #clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train, y_train)  \n",
    "    \n",
    "    y_pred1 = clf.predict(X_train)\n",
    "    RF_FDR1 = pd.DataFrame(y_train)\n",
    "    RF_FDR1['score'] = clf.predict_proba(X_train)[:,1]\n",
    "    RF_FDR1['pred_y'] = y_pred1\n",
    "    RF_FDR1 = RF_FDR1.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    \n",
    "    y_pred2 = clf.predict(X_test)\n",
    "    RF_FDR2 = pd.DataFrame(y_test)\n",
    "    RF_FDR2['score'] = clf.predict_proba(X_test)[:,1]\n",
    "    RF_FDR2['pred_y'] = y_pred2\n",
    "    RF_FDR2 = RF_FDR2.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    y_pred3 = clf.predict(x_ood)\n",
    "    RF_FDR3 = pd.DataFrame(y_ood)\n",
    "    RF_FDR3['score'] = clf.predict_proba(x_ood)[:,1]\n",
    "    RF_FDR3['pred_y'] = y_pred3\n",
    "    RF_FDR3 = RF_FDR3.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    for j in range(1,101):\n",
    "        RF_OOD.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,0])/np.sum(RF_FDR3.iloc[:,0]))\n",
    "        RF_OOD_FD.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,0]))\n",
    "        RF_OOD_FP.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,:]['pred_y']-RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,:]['Fraud']==1))\n",
    "        RF_train.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,0])/np.sum(RF_FDR1.iloc[:,0]))\n",
    "        RF_train_FD.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,0]))\n",
    "        RF_train_FP.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,:]['pred_y']-RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,:]['Fraud']==1))\n",
    "        RF_test.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,0])/np.sum(RF_FDR2.iloc[:,0]))\n",
    "        RF_test_FD.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,0]))\n",
    "        RF_test_FP.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,:]['pred_y']-RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,:]['Fraud']==1))\n",
    "    RF_train_total.append(RF_train)\n",
    "    RF_train_FPT.append(RF_train_FP)\n",
    "    RF_train_FDT.append(RF_train_FD)                      \n",
    "    RF_test_total.append(RF_test)\n",
    "    RF_test_FPT.append(RF_test_FP)\n",
    "    RF_test_FDT.append(RF_test_FD)\n",
    "    RF_OOD_total.append(RF_OOD)\n",
    "    RF_OOD_FPT.append(RF_OOD_FP)\n",
    "    RF_OOD_FDT.append(RF_OOD_FD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Boosting Tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "RF_train_total=[]\n",
    "RF_train_FDT=[]\n",
    "RF_train_FPT=[]\n",
    "RF_test_total=[]\n",
    "RF_test_FDT=[]\n",
    "RF_test_FPT=[]\n",
    "RF_OOD_total=[]\n",
    "RF_OOD_FDT=[]\n",
    "RF_OOD_FPT=[]\n",
    "for i in range(10):\n",
    "    RF_train=[]\n",
    "    RF_train_FP=[]\n",
    "    RF_train_FD=[]\n",
    "    RF_test=[]\n",
    "    RF_test_FP=[]\n",
    "    RF_test_FD=[]\n",
    "    RF_OOD=[]\n",
    "    RF_OOD_FP=[]\n",
    "    RF_OOD_FD=[]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25)\n",
    "    \n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf.fit(X_train, y_train)  \n",
    "    \n",
    "    y_pred1 = clf.predict(X_train)\n",
    "    RF_FDR1 = pd.DataFrame(y_train)\n",
    "    RF_FDR1['score'] = clf.predict_proba(X_train)[:,1]\n",
    "    RF_FDR1['pred_y'] = y_pred1\n",
    "    RF_FDR1 = RF_FDR1.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    \n",
    "    y_pred2 = clf.predict(X_test)\n",
    "    RF_FDR2 = pd.DataFrame(y_test)\n",
    "    RF_FDR2['score'] = clf.predict_proba(X_test)[:,1]\n",
    "    RF_FDR2['pred_y'] = y_pred2\n",
    "    RF_FDR2 = RF_FDR2.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    y_pred3 = clf.predict(x_ood)\n",
    "    RF_FDR3 = pd.DataFrame(y_ood)\n",
    "    RF_FDR3['score'] = clf.predict_proba(x_ood)[:,1]\n",
    "    RF_FDR3['pred_y'] = y_pred3\n",
    "    RF_FDR3 = RF_FDR3.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    for j in range(1,101):\n",
    "        RF_OOD.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,0])/np.sum(RF_FDR3.iloc[:,0]))\n",
    "        RF_OOD_FD.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,0]))\n",
    "        RF_OOD_FP.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,:]['pred_y']-RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,:]['Fraud']==1))\n",
    "        RF_train.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,0])/np.sum(RF_FDR1.iloc[:,0]))\n",
    "        RF_train_FD.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,0]))\n",
    "        RF_train_FP.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,:]['pred_y']-RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,:]['Fraud']==1))\n",
    "        RF_test.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,0])/np.sum(RF_FDR2.iloc[:,0]))\n",
    "        RF_test_FD.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,0]))\n",
    "        RF_test_FP.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,:]['pred_y']-RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,:]['Fraud']==1))\n",
    "    RF_train_total.append(RF_train)\n",
    "    RF_train_FPT.append(RF_train_FP)\n",
    "    RF_train_FDT.append(RF_train_FD)                      \n",
    "    RF_test_total.append(RF_test)\n",
    "    RF_test_FPT.append(RF_test_FP)\n",
    "    RF_test_FDT.append(RF_test_FD)\n",
    "    RF_OOD_total.append(RF_OOD)\n",
    "    RF_OOD_FPT.append(RF_OOD_FP)\n",
    "    RF_OOD_FDT.append(RF_OOD_FD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "RF_train_total=[]\n",
    "RF_train_FDT=[]\n",
    "RF_train_FPT=[]\n",
    "RF_test_total=[]\n",
    "RF_test_FDT=[]\n",
    "RF_test_FPT=[]\n",
    "RF_OOD_total=[]\n",
    "RF_OOD_FDT=[]\n",
    "RF_OOD_FPT=[]\n",
    "for i in range(10):\n",
    "    RF_train=[]\n",
    "    RF_train_FP=[]\n",
    "    RF_train_FD=[]\n",
    "    RF_test=[]\n",
    "    RF_test_FP=[]\n",
    "    RF_test_FD=[]\n",
    "    RF_OOD=[]\n",
    "    RF_OOD_FP=[]\n",
    "    RF_OOD_FD=[]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25)\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)  \n",
    "    \n",
    "    y_pred1 = clf.predict(X_train)\n",
    "    RF_FDR1 = pd.DataFrame(y_train)\n",
    "    RF_FDR1['score'] = clf.predict_proba(X_train)[:,1]\n",
    "    RF_FDR1['pred_y'] = y_pred1\n",
    "    RF_FDR1 = RF_FDR1.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    \n",
    "    y_pred2 = clf.predict(X_test)\n",
    "    RF_FDR2 = pd.DataFrame(y_test)\n",
    "    RF_FDR2['score'] = clf.predict_proba(X_test)[:,1]\n",
    "    RF_FDR2['pred_y'] = y_pred2\n",
    "    RF_FDR2 = RF_FDR2.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    y_pred3 = clf.predict(x_ood)\n",
    "    RF_FDR3 = pd.DataFrame(y_ood)\n",
    "    RF_FDR3['score'] = clf.predict_proba(x_ood)[:,1]\n",
    "    RF_FDR3['pred_y'] = y_pred3\n",
    "    RF_FDR3 = RF_FDR3.sort_values(by='score',ascending=False)\n",
    "    \n",
    "    for j in range(1,101):\n",
    "        RF_OOD.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,0])/np.sum(RF_FDR3.iloc[:,0]))\n",
    "        RF_OOD_FD.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,0]))\n",
    "        RF_OOD_FP.append(np.sum(RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,:]['pred_y']-RF_FDR3.iloc[:round(len(RF_FDR3)*j/100)+1,:]['Fraud']==1))\n",
    "        RF_train.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,0])/np.sum(RF_FDR1.iloc[:,0]))\n",
    "        RF_train_FD.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,0]))\n",
    "        RF_train_FP.append(np.sum(RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,:]['pred_y']-RF_FDR1.iloc[:round(len(RF_FDR1)*j/100)+1,:]['Fraud']==1))\n",
    "        RF_test.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,0])/np.sum(RF_FDR2.iloc[:,0]))\n",
    "        RF_test_FD.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,0]))\n",
    "        RF_test_FP.append(np.sum(RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,:]['pred_y']-RF_FDR2.iloc[:round(len(RF_FDR2)*j/100)+1,:]['Fraud']==1))\n",
    "    RF_train_total.append(RF_train)\n",
    "    RF_train_FPT.append(RF_train_FP)\n",
    "    RF_train_FDT.append(RF_train_FD)                      \n",
    "    RF_test_total.append(RF_test)\n",
    "    RF_test_FPT.append(RF_test_FP)\n",
    "    RF_test_FDT.append(RF_test_FD)\n",
    "    RF_OOD_total.append(RF_OOD)\n",
    "    RF_OOD_FPT.append(RF_OOD_FP)\n",
    "    RF_OOD_FDT.append(RF_OOD_FD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data for visualization\n",
    "#results\n",
    "RF_train_total_df = pd.DataFrame(RF_train_total)\n",
    "RF_test_total_df = pd.DataFrame(RF_test_total)\n",
    "RF_OOD_total_df = pd.DataFrame(RF_OOD_total)\n",
    "RF_train_FDT_df = pd.DataFrame(RF_train_FDT)\n",
    "RF_test_FDT_df = pd.DataFrame(RF_test_FDT)\n",
    "RF_OOD_FDT_df = pd.DataFrame(RF_OOD_FDT)\n",
    "RF_train_FPT_df = pd.DataFrame(RF_train_FPT)\n",
    "RF_test_FPT_df = pd.DataFrame(RF_test_FPT)\n",
    "RF_OOD_FPT_df = pd.DataFrame(RF_OOD_FPT)\n",
    "\n",
    "\n",
    "train_FDT=RF_train_FDT_df.mean(axis=0)*2000\n",
    "test_FDT=RF_test_FDT_df.mean(axis=0)*2000\n",
    "OOD_FDT=RF_OOD_FDT_df.mean(axis=0)*2000\n",
    "train_FPT=RF_train_FPT_df.mean(axis=0)*50\n",
    "test_FPT=RF_test_FPT_df.mean(axis=0)*50\n",
    "OOD_FPT=RF_OOD_FPT_df.mean(axis=0)*50\n",
    "\n",
    "#for graph\n",
    "rf_results = pd.DataFrame()\n",
    "rf_results['train_FDT']=train_FDT\n",
    "rf_results['test_FDT']=test_FDT\n",
    "rf_results['OOD_FDT']=OOD_FDT\n",
    "rf_results['train_FPT']=train_FPT\n",
    "rf_results['test_FPT']=test_FPT\n",
    "rf_results['OOD_FPT']=OOD_FPT\n",
    "print(rf_results)\n",
    "\n",
    "# for table\n",
    "rf_table = pd.DataFrame()\n",
    "rf_table['train']=RF_train_total_df.iloc[:,2]\n",
    "rf_table['test']=RF_test_total_df.iloc[:,2]\n",
    "rf_table['OOD']=RF_OOD_total_df.iloc[:,2]\n",
    "rf_table.loc[10]=rf_table.mean()\n",
    "print(rf_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
